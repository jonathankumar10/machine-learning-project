{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Final Project\n",
    "## Topic\n",
    "Create an NLP based model to understand the polarity of review and estimate a rating based on the review provided to a movie. Post we train a model to identify the polarity of review we will try to create a regression or classification model to map the review to a rating from range of 1-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and formating of input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# creating new folders for sorting the ratings\n",
    "for i in range(1,11):\n",
    "    try:\n",
    "        os.mkdir(f'./Cleaned_dataset/ratings_' + str(i))\n",
    "    except:\n",
    "        files = glob.glob('./Cleaned_dataset/ratings_' + str(i)+'/*')\n",
    "        os.remove(files)\n",
    "\n",
    "# filtering and copying negative reviews\n",
    "neg_path = './Project Dataset/aclImdb/train/neg/'\n",
    "neg_files = os.listdir(neg_path)\n",
    "flip = False\n",
    "for file_name in neg_files:\n",
    "    try:\n",
    "        # copying review to their respective rating folder\n",
    "        rating = file_name.split('_')[1].split('.')[0]\n",
    "        if rating == '4':\n",
    "            if(not flip):\n",
    "                shutil.copy2(neg_path + file_name,'./Cleaned_dataset/ratings_4/'+ file_name)\n",
    "                flip = True\n",
    "            else:\n",
    "                shutil.copy2(neg_path + file_name,'./Cleaned_dataset/ratings_5/'+ file_name)\n",
    "                flip = False\n",
    "        else:\n",
    "            shutil.copy2(neg_path + file_name,'./Cleaned_dataset/ratings_' + rating +'/' + file_name)\n",
    "    except:\n",
    "        print(neg_path + file_name)\n",
    "        \n",
    "# filtering and copying positive reviews\n",
    "pos_path = './Project Dataset/aclImdb/train/pos/'\n",
    "pos_files = os.listdir(pos_path)\n",
    "flip = False\n",
    "for file_name in pos_files:\n",
    "    try:\n",
    "        # copying review to their respective rating folder\n",
    "        rating = file_name.split('_')[1].split('.')[0]\n",
    "        if rating == '7':\n",
    "            if(not flip):\n",
    "                shutil.copy2(pos_path + file_name,'./Cleaned_dataset/ratings_7/' + file_name)\n",
    "                flip = True\n",
    "            else:\n",
    "                shutil.copy2(pos_path + file_name,'./Cleaned_dataset/ratings_6/' + file_name)\n",
    "                flip = False\n",
    "        else:\n",
    "            shutil.copy2(pos_path + file_name,'./Cleaned_dataset/ratings_' + rating + '/' + file_name)\n",
    "    except:\n",
    "        print(pos_path + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data in to data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_set = pd.DataFrame(columns=['Rating','Review'])\n",
    "ratings_folder_path = os.listdir('./Cleaned_dataset/')\n",
    "for folder in ratings_folder_path:\n",
    "    rating = folder.split('_')\n",
    "    review_files = os.listdir('./Cleaned_dataset/' + folder + '/')\n",
    "    for review in review_files:\n",
    "        try:\n",
    "            fp = open('./Cleaned_dataset/' + folder + '/' + review,'r')\n",
    "            review_data = fp.read()\n",
    "            rating_number = rating[1]\n",
    "            data_set = data_set.append(pd.Series([rating_number,review_data], index=data_set.columns),ignore_index=True)\n",
    "            fp.close()\n",
    "        except:\n",
    "            # try except to deal with error in file reading due to codec issues\n",
    "            pass\n",
    "\n",
    "print(data_set.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing data_set\n",
    "### Removing punctuations, Stop words and Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import nltk\n",
    "# download wordnet if required\n",
    "# nltk.download('wordnet')\n",
    "# download stopwords if required\n",
    "# nltk.download('stopwords')\n",
    "# loading English stop words  \n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Basic cleaning\n",
    "def cleanAndTokenize(review):\n",
    "    # removing punctuations\n",
    "    non_punc_words = \"\".join([character for character in review if character not in string.punctuation])\n",
    "    \n",
    "    non_punc_words = non_punc_words.strip()\n",
    "    \n",
    "    # tokenizing reviews\n",
    "    list_of_token = re.split('\\W+',non_punc_words)\n",
    "    \n",
    "    # removing stop words\n",
    "    tokens = [word for word in list_of_token if word not in stop_words]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# converting words to lower case.\n",
    "data_set['Review'] = data_set['Review'].apply(lambda review : cleanAndTokenize(str(review).lower()))\n",
    "\n",
    "# using nltk's wordnet lemmatizer\n",
    "word_net_lemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_data(token_list):\n",
    "    tokens = [word_net_lemma.lemmatize(word) for word in token_list]\n",
    "    return tokens\n",
    "\n",
    "data_set['Review'] = data_set['Review'].apply(lambda review : lemmatize_data(review))\n",
    "\n",
    "# un-comment to write data to file\n",
    "# data_set.to_csv('./Cleaned_dataset/tokenized_words.csv',index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing reviews using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rating                                             Review\n",
      "0       1  ['sorry', 'everyone', 'know', 'supposed', 'art...\n",
      "1       1  ['little', 'parent', 'took', 'along', 'theater...\n",
      "2       1  ['film', 'mediocre', 'best', 'angie', 'harmon'...\n",
      "3       1  ['film', 'one', 'giant', 'pant', 'load', 'paul...\n",
      "4       1  ['movie', 'must', 'line', 'boring', 'movie', '...\n",
      "     0   00  000  0000000000001  00001  00015  000s  001  003830  006  ...  \\\n",
      "0  0.0  0.0  0.0            0.0    0.0    0.0   0.0  0.0     0.0  0.0  ...   \n",
      "1  0.0  0.0  0.0            0.0    0.0    0.0   0.0  0.0     0.0  0.0  ...   \n",
      "2  0.0  0.0  0.0            0.0    0.0    0.0   0.0  0.0     0.0  0.0  ...   \n",
      "3  0.0  0.0  0.0            0.0    0.0    0.0   0.0  0.0     0.0  0.0  ...   \n",
      "4  0.0  0.0  0.0            0.0    0.0    0.0   0.0  0.0     0.0  0.0  ...   \n",
      "\n",
      "    âº   â½   â¾  âžiâžek    ã  ã¼ber  ãœvegtigris  ãšxtase    ï  œat  \n",
      "0  0.0  0.0  0.0      0.0  0.0    0.0          0.0      0.0  0.0  0.0  \n",
      "1  0.0  0.0  0.0      0.0  0.0    0.0          0.0      0.0  0.0  0.0  \n",
      "2  0.0  0.0  0.0      0.0  0.0    0.0          0.0      0.0  0.0  0.0  \n",
      "3  0.0  0.0  0.0      0.0  0.0    0.0          0.0      0.0  0.0  0.0  \n",
      "4  0.0  0.0  0.0      0.0  0.0    0.0          0.0      0.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 67517 columns]\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "# using nltk's wordnet lemmatizer\n",
    "word_net_lemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "data_set = pd.read_csv('./Cleaned_dataset/tokenized_words.csv')\n",
    "\n",
    "print(data_set.head())\n",
    "\n",
    "def clean_review(review):\n",
    "    non_punc_words = \"\".join([character for character in review if character not in string.punctuation])\n",
    "    list_of_token = re.split('\\W+',non_punc_words)\n",
    "    tokens = [word_net_lemma.lemmatize(word) for word in list_of_token]\n",
    "    return tokens\n",
    "\n",
    "tfidf_vectorize = TfidfVectorizer(analyzer=clean_review)\n",
    "vectorized_review = tfidf_vectorize.fit_transform(data_set['Review'])\n",
    "\n",
    "pickle.dump(tfidf_vectorize,open('./pickle_tfidf/tfidf.pickle','wb'))\n",
    "vectorized_review_df = pd.DataFrame(vectorized_review.toarray())\n",
    "vectorized_review_df.columns = tfidf_vectorize.get_feature_names()\n",
    "print(vectorized_review_df.head())\n",
    "\n",
    "pickle.dump(vectorized_review,open('./pickle_tfidf/vectorized_review.pickle','wb'))\n",
    "\n",
    "# un-comment to write data to file\n",
    "# data_set.to_csv('./Cleaned_dataset/vectorized_words.csv',index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the TFIDF object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "[[1.404389  ]\n",
      " [1.40673365]\n",
      " [1.41421356]\n",
      " ...\n",
      " [1.40625837]\n",
      " [1.37487303]\n",
      " [1.35379579]]\n",
      "      Rating\n",
      "5103      10\n",
      "   Rating\n",
      "0       1\n",
      "Wall time: 421 ms\n",
      "Parser   : 160 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "class_data = pd.read_csv('./Cleaned_dataset/tokenized_words.csv',usecols = ['Rating'])\n",
    "\n",
    "tfidf_object = pickle.load(open('./pickle_tfidf/tfidf.pickle','rb'))\n",
    "vectorized_review_obj = pickle.load(open('./pickle_tfidf/vectorized_review.pickle','rb'))\n",
    "new_q = '''stupid plot, I will give it really low ratings.'''\n",
    "new_in = tfidf_object.transform([new_q])\n",
    "print(new_in.todense())\n",
    "distances = euclidean_distances(vectorized_review_obj,new_in)\n",
    "print(distances)\n",
    "list_min_distance = np.where(distances == np.amin(distances))[:]\n",
    "for k in list_min_distance:\n",
    "    print(class_data.loc[k,:])\n",
    "\n",
    "# class_data = class_data.drop(class_data.index[list_max_distance])\n",
    "# print(class_data['Rating'].value_counts())\n",
    "# new_in = pd.DataFrame.sparse.from_spmatrix(new_in)\n",
    "# print(new_in.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
