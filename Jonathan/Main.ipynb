{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_dataset/final_train_dataset.csv')\n",
    "test_data = pd.read_csv('test_dataset/final_test_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 rows of train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns = train_data.columns.str.lower()\n",
    "test_data.columns = test_data.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "def cleanText(text):\n",
    "    \n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"'ve\", \" have\", text)\n",
    "    text = re.sub(r\"'ll\", \" will\", text)\n",
    "    text = re.sub(r\"'re\", \" are\", text)\n",
    "\n",
    "    text = re.sub(r\"[0-9]+\", ' ', text)\n",
    "    text = re.sub(r\"-\", ' ', text)\n",
    "    \n",
    "    \n",
    "    text = text.strip().lower()\n",
    "    \n",
    "\n",
    "    default_stop_words = set(stopwords.words('english'))\n",
    "    default_stop_words.difference_update({'no', 'not', 'nor', 'too', 'any'})\n",
    "    stop_words = default_stop_words.union({\"'m\", \"n't\", \"'d\", \"'re\", \"'s\",\n",
    "                                           'would','must',\"'ve\",\"'ll\",'may'})\n",
    "\n",
    "    word_list = word_tokenize(text)\n",
    "    filtered_list = [w for w in word_list if not w in stop_words]\n",
    "    text = ' '.join(filtered_list)\n",
    "    \n",
    "    text = re.sub(r\"'\", ' ', text)\n",
    "    \n",
    "   \n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((i, \" \") for i in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "    \n",
    "\n",
    "    text = ' '.join([w for w in text.split() if len(w)>1])\n",
    "\n",
    "    # Replace multiple space with one space\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    text = ''.join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def NormalizeWithPOS(text):\n",
    "    # Lemmatization & Stemming according to POS tagging\n",
    "\n",
    "    word_list = word_tokenize(text)\n",
    "    rev = []\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    stemmer = PorterStemmer() \n",
    "    for word, tag in pos_tag(word_list):\n",
    "        if tag.startswith('J'):\n",
    "            w = lemmatizer.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('V'):\n",
    "            w = lemmatizer.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('N'):\n",
    "            w = lemmatizer.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('R'):\n",
    "            w = lemmatizer.lemmatize(word, pos='r')\n",
    "        else:\n",
    "            w = word\n",
    "        w = stemmer.stem(w)\n",
    "        rev.append(w)\n",
    "    review = ' '.join(rev)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data['clean_reviews'] = train_data['reviews'].apply(cleanText)\n",
    "test_data['clean_reviews'] = test_data['reviews'].apply(cleanText)\n",
    "train_data['clean_reviews_normalized'] = train_data['clean_reviews'].apply(NormalizeWithPOS)\n",
    "test_data['clean_reviews_normalized'] = test_data['clean_reviews'].apply(NormalizeWithPOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickle_data/train_data_preLowFreq.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(train_data, pickle_file)\n",
    "with open('./pickle_data/test_data_preLowFreq.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(test_data, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data from pickled data for Low freqency word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickle_data/train_data_preLowFreq.pkl', 'rb') as pickle_file:\n",
    "    train_data = pickle.load(pickle_file)\n",
    "with open('./pickle_data/test_data_preLowFreq.pkl', 'rb') as pickle_file:\n",
    "    test_data = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Frequency Words of Train Data for BOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wopat       4\n",
      "gorier      4\n",
      "briberi     4\n",
      "ashleigh    4\n",
      "crossbre    4\n",
      "           ..\n",
      "chancho     1\n",
      "unredem     1\n",
      "helix       1\n",
      "alto        1\n",
      "ãlvaro     1\n",
      "Length: 31145, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "freq_train1 = pd.Series(' '.join(train_data['clean_reviews_normalized']).split()).value_counts()\n",
    "less_five_freq_train1 = freq_train1[(freq_train1 <5)]\n",
    "print(less_five_freq_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wheezing        4\n",
      "salina          4\n",
      "relished        4\n",
      "unrecognized    4\n",
      "lemoine         4\n",
      "               ..\n",
      "callousness     1\n",
      "tarkosvky       1\n",
      "numerical       1\n",
      "brennanâ…       1\n",
      "herâ…but        1\n",
      "Length: 45829, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "freq_train2 = pd.Series(' '.join(train_data['clean_reviews']).split()).value_counts()\n",
    "less_five_freq_train2 = freq_train2[(freq_train2 <5)]\n",
    "print(less_five_freq_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low Frequency Words of Test Data for BOW¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bavaria       4\n",
      "roxann        4\n",
      "firehous      4\n",
      "elmo          4\n",
      "malaya        4\n",
      "             ..\n",
      "emtpi         1\n",
      "jewess        1\n",
      "lowerclass    1\n",
      "beligium      1\n",
      "dogey         1\n",
      "Length: 30688, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "freq_test3 = pd.Series(' '.join(test_data['clean_reviews_normalized']).split()).value_counts()\n",
    "less_five_freq_test3 = freq_test3[(freq_test3 <5)]\n",
    "print(less_five_freq_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senility     4\n",
      "rosenthal    4\n",
      "wack         4\n",
      "unfazed      4\n",
      "tarkowsky    4\n",
      "            ..\n",
      "achenbach    1\n",
      "ugghhhh      1\n",
      "mussing      1\n",
      "iistening    1\n",
      "envies       1\n",
      "Length: 45428, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "freq_test4 = pd.Series(' '.join(test_data['clean_reviews']).split()).value_counts()\n",
    "less_five_freq_test4 = freq_test4[(freq_test4 <5)]\n",
    "print(less_five_freq_test4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove words with frequency less than 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_data['clean_reviews_normalized'] = train_data['clean_reviews_normalized'].apply(lambda x: ' '.join(x for x in x.split() if x not in less_five_freq_train1))\n",
    "test_data['clean_reviews_normalized'] = test_data['clean_reviews_normalized'].apply(lambda x: ' '.join(x for x in x.split() if x not in less_five_freq_test3))\n",
    "\n",
    "\n",
    "train_data['clean_reviews'] = train_data['clean_reviews'].apply(lambda x: ' '.join(x for x in x.split() if x not in less_five_freq_train2))\n",
    "test_data['clean_reviews'] = test_data['clean_reviews'].apply(lambda x: ' '.join(x for x in x.split() if x not in less_five_freq_test4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickle_data/train_data_final.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(train_data, pickle_file)\n",
    "with open('./pickle_data/test_data_final.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(test_data, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A visual comparison of different cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A review example of dataset before cleaning:\", end=\"\\n\\n\")\n",
    "print(train_data.iloc[0]['reviews'], end='\\n\\n')\n",
    "\n",
    "print(\"clean_text:\")\n",
    "print(train_data.iloc[0]['clean_reviews'], end=\"\\n\\n\")\n",
    "\n",
    "print(\"clean_text_normalized:\")\n",
    "print(train_data.iloc[0]['clean_reviews_normalized'], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "training_features_normalized = vectorizer.fit_transform(train_data['clean_reviews_normalized'])\n",
    "testing_features_normalized = vectorizer.transform(test_data['clean_reviews_normalized'])\n",
    "\n",
    "training_features = vectorizer.fit_transform(train_data['clean_reviews'])\n",
    "testing_features = vectorizer.transform(test_data['clean_reviews'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling count vectorized data for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./pickle_data/training_features_normalized.pkl', 'wb') as pickle_file:\n",
    "#     pickle.dump(training_features_normalized, pickle_file)\n",
    "# with open('./pickle_data/testing_features_normalized.pkl', 'wb') as pickle_file:\n",
    "#     pickle.dump(testing_features_normalized, pickle_file)\n",
    "# with open('./pickle_data/training_features.pkl', 'wb') as pickle_file:\n",
    "#     pickle.dump(training_features, pickle_file)\n",
    "# with open('./pickle_data/testing_features.pkl', 'wb') as pickle_file:\n",
    "#     pickle.dump(testing_features, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./pickle_data/training_features_normalized.pkl', 'rb') as pickle_file:\n",
    "#     training_features_normalized = pickle.load(pickle_file)\n",
    "# with open('./pickle_data/testing_features_normalized.pkl', 'rb') as pickle_file:\n",
    "#     testing_features_normalized = pickle.load(pickle_file)\n",
    "# with open('./pickle_data/training_features.pkl', 'rb') as pickle_file:\n",
    "#     training_features = pickle.load(pickle_file)\n",
    "# with open('./pickle_data/testing_features.pkl', 'rb') as pickle_file:\n",
    "#     testing_features = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testing_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_features_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testing_features_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResult(y_pred, y_prob):\n",
    "    acc = accuracy_score(test_data[\"class\"], y_pred)\n",
    "    # Result\n",
    "    print(\"Accuracy: {:.2f}\".format(acc*100),end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./pickle_data/train_data.pkl', 'rb') as pickle_file:\n",
    "#     train_data = pickle.load(pickle_file)\n",
    "# with open('./pickle_data/test_data.pkl', 'rb') as pickle_file:\n",
    "#     test_data = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_train = pd.Series(' '.join(train_data['clean_reviews_normalized']).split()).value_counts()\n",
    "# less_five_freq_train = freq_train[(freq_train <5)]\n",
    "# print(less_five_freq_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq_test = pd.Series(' '.join(test_data['clean_reviews_normalized']).split()).value_counts()\n",
    "# less_five_freq_test = freq_test[(freq_test <5)]\n",
    "# print(less_five_freq_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# train_data['clean_reviews_normalized'] = train_data['clean_reviews_normalized'].apply(lambda x: ' '.join(x for x in x.split() if x not in less_five_freq_train))\n",
    "# test_data['clean_reviews_normalized'] = test_data['clean_reviews_normalized'].apply(lambda x: ' '.join(x for x in x.split() if x not in less_five_freq_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "model_normalized = RandomForestClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "model.fit(training_features, train_data[\"class\"])\n",
    "model_normalized.fit(training_features_normalized, train_data[\"class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_normalized = model_normalized.predict(testing_features_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_normalized_1 = model_normalized.predict(testing_features_normalized[999])\n",
    "predict_1 = model.predict(testing_features[999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_normalized_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_prob = model_normalized.predict_proba(testing_features_normalized)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printResult(predict, predict_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(test_data['class'],predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(test_data['class'],predict_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Input and modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take in user input and convert it to a list followed that by converting it to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = input('Please enter a review: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = [Input]\n",
    "input_df = DataFrame(Input,columns=['reviews'])\n",
    "input_df['clean_reviews'] = input_df['reviews'].apply(cleanText)\n",
    "input_df['clean_reviews_normalized'] = input_df['clean_reviews'].apply(NormalizeWithPOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# training_features_normalized = vectorizer.fit_transform(train_data['clean_reviews_normalized'])\n",
    "# testing_features_normalized = vectorizer.transform(test_data['clean_reviews_normalized'])\n",
    "\n",
    "\n",
    "# training_features = vectorizer.fit_transform(train_data['clean_reviews'])\n",
    "# testing_features = vectorizer.transform(test_data['clean_reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_testing_features_normal = vectorizer.transform(input_df['clean_reviews_normalized'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_testing_features = vectorizer.transform(input_df['clean_reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_testing_features_normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_testing_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_normal = model.predict(input_testing_features_normal)\n",
    "predict = model.predict(input_testing_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_normal[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
